/**
 * Copyright (c) 2025 ESSENTIAL SERVICES
 *
 * This source code is licensed under the Business Source License 1.1
 * found in the LICENSE.md file in the root directory of this source tree.
 */

/* eslint-disable max-lines -- Self-contained utility script */

/**
 * Spec Quality Analyzer
 *
 * Analyzes E2E test files (.spec.ts) for quality and generates a SPEC-PROGRESS.md file
 * that serves as a single source of truth for test coverage and specification review.
 *
 * Quality checks:
 * - Test naming (descriptive, follows conventions)
 * - Spec IDs (APP-*, API-*, STATIC-*, etc.)
 * - GIVEN/WHEN/THEN structure in comments
 * - Test organization (@spec vs @regression tags)
 * - Coverage gaps and missing behaviors
 * - Header "Spec Count: X" matches actual @spec count
 *
 * Usage:
 *   bun run scripts/check-progress.ts [options]
 *
 * Options:
 *   --filter=pattern   Filter files by regex pattern (e.g., --filter=version)
 *   --fixme            Show list of fixme tests for prioritization
 *   --fix              Auto-fix header count mismatches
 *   --no-error         Don't exit with error code on quality issues
 *   --verify-progress  Cross-reference with GitHub PRs/issues to detect discrepancies
 *   --update-stories   Update user story files with test status column (‚úÖ/‚è≥/‚ùì)
 *
 * Output:
 *   - SPEC-PROGRESS.md: Reviewable specification state document
 *   - Console: Quality report with issues and suggestions
 *
 * Examples:
 *   bun run progress                    # Full analysis
 *   bun run progress --filter=version   # Analyze only version specs
 *   bun run progress --fixme            # Show fixme tests to implement next
 *   bun run progress --fix              # Auto-fix header count mismatches
 *   bun run progress --update-stories   # Update user stories with test status
 */

import { readdir, readFile, writeFile, unlink, access } from 'node:fs/promises'
import { join, relative, basename } from 'node:path'
import * as prettier from 'prettier'

// =============================================================================
// Types
// =============================================================================

interface SpecTest {
  id: string | null
  name: string
  tag: '@spec' | '@regression' | null
  hasGiven: boolean
  hasWhen: boolean
  hasThen: boolean
  lineNumber: number
  isFixme: boolean
  rawContent: string
}

interface SpecFile {
  path: string
  relativePath: string
  feature: string
  tests: SpecTest[]
  issues: QualityIssue[]
  metadata: {
    hasDescribe: boolean
    describeLabel: string | null
    totalTests: number
    specTests: number
    regressionTests: number
    fixmeTests: number
    passingTests: number
  }
}

interface QualityIssue {
  type: 'error' | 'warning' | 'suggestion'
  code: string
  message: string
  line?: number
  testId?: string
}

interface SpecState {
  generatedAt: string
  summary: {
    totalFiles: number
    totalTests: number
    totalSpecs: number
    totalRegressions: number
    totalFixme: number
    totalPassing: number
    qualityScore: number
    duplicateSpecIds: number
    orphanedSpecIds: number
    issuesByType: {
      errors: number
      warnings: number
      suggestions: number
    }
  }
  files: SpecFile[]
  coverageGaps: CoverageGap[]
  duplicateSpecIds: DuplicateSpecId[]
  orphanedSpecIds: OrphanedSpecId[]
  tddAutomation: TDDAutomationStats
  userStoryMetrics: UserStoryMetrics | null
}

interface CoverageGap {
  feature: string
  missingBehaviors: string[]
  suggestion: string
}

interface DuplicateSpecId {
  specId: string
  locations: Array<{
    file: string
    line: number
    testName: string
  }>
}

interface TDDAutomationStats {
  totalFixed: number
  fixedByPipeline: number
  fixedManually: number
  fixedLast24h: number
  fixedLast7d: number
  fixedLast30d: number
  avgFixesPerDay: number
  estimatedDaysRemaining: number | null
  estimatedCompletionDate: string | null
  recentFixes: Array<{
    specId: string
    date: string
    commitHash: string
    source: 'tdd-pipeline' | 'manual'
  }>
}

interface GitHubReference {
  type: 'pr' | 'issue' | 'commit'
  number?: number
  hash?: string
  title: string
  specId: string
  date: string
  url?: string
}

interface ProgressDiscrepancy {
  specId: string
  status: 'missing-in-codebase' | 'still-fixme' | 'fixed-but-not-tracked'
  references: GitHubReference[]
  suggestion: string
}

interface VerificationResult {
  allReferences: GitHubReference[]
  discrepancies: ProgressDiscrepancy[]
  fixedSpecIds: Set<string>
}

interface DomainSummary {
  domain: string
  totalFiles: number
  totalTests: number
  passingTests: number
  fixmeTests: number
  progressPercent: number
  status: 'üü¢' | 'üü°' | 'üî¥'
  files: SpecFile[]
}

// =============================================================================
// User Story Tracking Types
// =============================================================================

interface AcceptanceCriterion {
  id: string // e.g., "AC-001"
  criterion: string
  specTestId: string | null // e.g., "API-AUTH-SIGN-UP-EMAIL-001" or null if not linked
  schema: string
  status: 'complete' | 'partial' | 'not-started'
}

interface UserStory {
  id: string // e.g., "US-AUTH-METHOD-DEV-001"
  title: string
  story: string
  status: 'complete' | 'partial' | 'not-started'
  acceptanceCriteria: AcceptanceCriterion[]
  filePath: string
  domain: string
  featureArea: string
  role: string
}

interface UserStoryMetrics {
  totalStories: number
  totalCriteria: number
  // Stories NOT linked to any spec IDs
  storiesToSpecify: UserStory[]
  storiesToSpecifyPercent: number
  // Stories linked to spec IDs with .fixme() status
  storiesSpecifiedNotImplemented: UserStory[]
  storiesSpecifiedNotImplementedPercent: number
  // Stories linked to passing spec IDs
  storiesSpecifiedAndImplemented: UserStory[]
  storiesSpecifiedAndImplementedPercent: number
  // All stories organized by domain
  byDomain: Map<string, UserStory[]>
}

interface OrphanedSpecId {
  specId: string
  file: string
  line: number
  testName: string
}

// =============================================================================
// Constants
// =============================================================================

const SPEC_ID_PATTERN = /^([A-Z]+-[A-Z0-9-]+-(?:\d{3}|REGRESSION))/
const GIVEN_PATTERN = /\/\/\s*GIVEN/i
const WHEN_PATTERN = /\/\/\s*(WHEN|GIVEN\/WHEN|WHEN\/THEN)/i
const THEN_PATTERN = /\/\/\s*(THEN|WHEN\/THEN)/i
// Implicit THEN: expect().rejects or expect().resolves patterns (assertion is built into the expect)
const IMPLICIT_THEN_PATTERN = /\.\s*(rejects|resolves)\s*\.\s*(toThrow|toBe|toEqual|toMatch)/
const TEST_PATTERN = /test\s*\(\s*['"`]([^'"`]+)['"`]/g
const TEST_FIXME_PATTERN = /test\.fixme\s*\(\s*['"`]([^'"`]+)['"`]/g
const DESCRIBE_PATTERN = /test\.describe\s*\(\s*['"`]([^'"`]+)['"`]/
const TAG_PATTERN = /\{\s*tag:\s*['"`](@spec|@regression)['"`]\s*\}/
const TODO_PATTERN = /\/\/\s*TODO[:\s](.+?)(?:\n|$)/gi

const SPECS_DIR = join(process.cwd(), 'specs')
const OUTPUT_FILE = join(process.cwd(), 'SPEC-PROGRESS.md')
const OLD_OUTPUT_FILE = join(process.cwd(), 'SPEC-STATE.md') // For migration
const README_FILE = join(process.cwd(), 'README.md')
const USER_STORIES_DIR = join(process.cwd(), 'docs/user-stories')

// User Story parsing patterns
const USER_STORY_STATUS_PATTERN = /\*\*Status\*\*:\s*`\[([x~\s])\]`/
// Match spec test IDs in acceptance criteria tables
// Format: | ID | Criterion | E2E Spec | (optional: Status) |
// Example: | AC-001 | User receives 200 OK... | `API-AUTH-SIGN-UP-EMAIL-001` | ‚úÖ |
// Note: The 4th Status column is optional (added by --update-stories flag)
const ACCEPTANCE_CRITERIA_TABLE_ROW_PATTERN =
  /^\|\s*(AC-\d{3})\s*\|\s*([^|]+)\s*\|\s*`?([A-Z]+-[A-Z0-9-]+-(?:\d{3}|REGRESSION))?`?\s*\|(?:\s*[^|]*\s*\|)?$/gm
// Domain/feature/role from file metadata
const DOMAIN_PATTERN = />\s*\*\*Domain\*\*:\s*(\S+)/
const FEATURE_AREA_PATTERN = />\s*\*\*Feature Area\*\*:\s*(\S+)/
const ROLE_PATTERN = />\s*\*\*Role\*\*:\s*(.+)/

// =============================================================================
// Analysis Functions
// =============================================================================

async function findSpecFiles(dir: string): Promise<string[]> {
  const files: string[] = []

  async function walk(currentDir: string) {
    const entries = await readdir(currentDir, { withFileTypes: true })

    for (const entry of entries) {
      const fullPath = join(currentDir, entry.name)

      if (entry.isDirectory()) {
        // Skip __snapshots__ directories
        if (entry.name !== '__snapshots__') {
          await walk(fullPath)
        }
      } else if (entry.name.endsWith('.spec.ts')) {
        files.push(fullPath)
      }
    }
  }

  await walk(dir)
  return files.sort()
}

function extractTests(content: string, _filePath: string): SpecTest[] {
  const tests: SpecTest[] = []

  // Find all test declarations
  let match: RegExpExecArray | null

  // Reset regex state
  TEST_PATTERN.lastIndex = 0
  TEST_FIXME_PATTERN.lastIndex = 0

  // Find regular tests
  while ((match = TEST_PATTERN.exec(content)) !== null) {
    const testName = match[1]
    if (!testName) continue
    const matchIndex = match.index

    // Find line number
    const beforeMatch = content.substring(0, matchIndex)
    const lineNumber = beforeMatch.split('\n').length

    // Extract the full test block - need to find the test function body, not just options
    // Test structure: test('name', { options }, async () => { ... })
    // We need to find the outermost closing paren that matches test(
    // IMPORTANT: Skip parentheses inside strings, comments, AND regex literals
    const testStart = matchIndex
    let parenCount = 0
    let testEnd = testStart
    let foundFirstParen = false
    let inString: string | null = null // Track which quote char started the string
    let inSingleLineComment = false // Track // comments
    let inMultiLineComment = false // Track /* */ comments
    let inRegex = false // Track /regex/ literals
    let lastSignificantChar = '' // Track last non-whitespace char for regex detection

    // Characters that typically precede a regex literal (not division)
    const regexPreceders = new Set([
      '(',
      ',',
      '=',
      ':',
      '[',
      '!',
      '&',
      '|',
      '?',
      '{',
      '}',
      ';',
      '\n',
    ])

    for (let i = testStart; i < content.length; i++) {
      const char = content[i]
      if (char === undefined) continue // Guard for TypeScript strict mode
      const prevChar = i > 0 ? content[i - 1] : ''
      const nextChar = i < content.length - 1 ? content[i + 1] : ''

      // Handle single-line comment end (newline)
      if (inSingleLineComment && char === '\n') {
        inSingleLineComment = false
        lastSignificantChar = '\n'
        continue
      }

      // Handle multi-line comment end
      if (inMultiLineComment && char === '*' && nextChar === '/') {
        inMultiLineComment = false
        i++ // Skip the '/'
        continue
      }

      // Handle regex end (unescaped /)
      if (inRegex && char === '/' && prevChar !== '\\') {
        inRegex = false
        lastSignificantChar = '/'
        continue
      }

      // Skip everything inside comments
      if (inSingleLineComment || inMultiLineComment) {
        continue
      }

      // Skip everything inside regex (but handle escaped chars)
      if (inRegex) {
        if (char === '\\' && nextChar) {
          i++ // Skip the escaped character
        }
        continue
      }

      // Detect comment or regex start (only when not in string)
      if (inString === null) {
        if (char === '/' && nextChar === '/') {
          inSingleLineComment = true
          i++ // Skip the second '/'
          continue
        }
        if (char === '/' && nextChar === '*') {
          inMultiLineComment = true
          i++ // Skip the '*'
          continue
        }
        // Detect regex literal: / followed by non-/ and non-* after a regex-preceding char
        if (
          char === '/' &&
          nextChar !== '/' &&
          nextChar !== '*' &&
          regexPreceders.has(lastSignificantChar)
        ) {
          inRegex = true
          continue
        }
      }

      // Handle string boundaries (skip escaped quotes) - only when not in comment
      if ((char === '"' || char === "'" || char === '`') && prevChar !== '\\') {
        if (inString === null) {
          inString = char // Enter string
        } else if (inString === char) {
          inString = null // Exit string
        }
        // Don't continue - we still need to check parenCount at end of loop
      }

      // Only count parentheses outside of strings, comments, and regex
      if (inString === null) {
        if (char === '(') {
          parenCount++
          foundFirstParen = true
        } else if (char === ')') {
          parenCount--
        }
      }

      // Track last significant (non-whitespace) character for regex detection
      if (inString === null && !/\s/.test(char)) {
        lastSignificantChar = char
      }

      if (foundFirstParen && parenCount === 0) {
        testEnd = i + 1
        break
      }
    }

    const rawContent = content.substring(testStart, testEnd)

    // Check for GIVEN/WHEN/THEN
    const hasGiven = GIVEN_PATTERN.test(rawContent)
    const hasWhen = WHEN_PATTERN.test(rawContent)
    // THEN can be explicit comment or implicit via expect().rejects/.resolves pattern
    const hasThen = THEN_PATTERN.test(rawContent) || IMPLICIT_THEN_PATTERN.test(rawContent)

    // Extract spec ID from test name
    const idMatch = testName.match(SPEC_ID_PATTERN)
    const id = idMatch?.[1] ?? null

    // Check for tag
    const tagMatch = rawContent.match(TAG_PATTERN)
    const tag = tagMatch ? (tagMatch[1] as '@spec' | '@regression') : null

    // Check if it's a fixme test
    const isFixme = false

    tests.push({
      id,
      name: testName as string,
      tag,
      hasGiven,
      hasWhen,
      hasThen,
      lineNumber,
      isFixme,
      rawContent,
    })
  }

  // Find fixme tests
  while ((match = TEST_FIXME_PATTERN.exec(content)) !== null) {
    const testName = match[1]
    if (!testName) continue
    const matchIndex = match.index

    const beforeMatch = content.substring(0, matchIndex)
    const lineNumber = beforeMatch.split('\n').length

    // Extract the full test block using parenthesis counting (same as regular tests)
    // IMPORTANT: Skip parentheses inside strings, comments, AND regex literals to avoid miscounting
    const testStart = matchIndex
    let parenCount = 0
    let testEnd = testStart
    let foundFirstParen = false
    let inString: string | null = null // Track which quote char started the string
    let inSingleLineComment = false // Track // comments
    let inMultiLineComment = false // Track /* */ comments
    let inRegex = false // Track /regex/ literals
    let lastSignificantChar = '' // Track last non-whitespace char for regex detection

    // Characters that typically precede a regex literal (not division)
    const regexPreceders = new Set([
      '(',
      ',',
      '=',
      ':',
      '[',
      '!',
      '&',
      '|',
      '?',
      '{',
      '}',
      ';',
      '\n',
    ])

    for (let i = testStart; i < content.length; i++) {
      const char = content[i]
      if (char === undefined) continue // Guard for TypeScript strict mode
      const prevChar = i > 0 ? content[i - 1] : ''
      const nextChar = i < content.length - 1 ? content[i + 1] : ''

      // Handle single-line comment end (newline)
      if (inSingleLineComment && char === '\n') {
        inSingleLineComment = false
        lastSignificantChar = '\n'
        continue
      }

      // Handle multi-line comment end
      if (inMultiLineComment && char === '*' && nextChar === '/') {
        inMultiLineComment = false
        i++ // Skip the '/'
        continue
      }

      // Handle regex end (unescaped /)
      if (inRegex && char === '/' && prevChar !== '\\') {
        inRegex = false
        lastSignificantChar = '/'
        continue
      }

      // Skip everything inside comments
      if (inSingleLineComment || inMultiLineComment) {
        continue
      }

      // Skip everything inside regex (but handle escaped chars)
      if (inRegex) {
        if (char === '\\' && nextChar) {
          i++ // Skip the escaped character
        }
        continue
      }

      // Detect comment or regex start (only when not in string)
      if (inString === null) {
        if (char === '/' && nextChar === '/') {
          inSingleLineComment = true
          i++ // Skip the second '/'
          continue
        }
        if (char === '/' && nextChar === '*') {
          inMultiLineComment = true
          i++ // Skip the '*'
          continue
        }
        // Detect regex literal: / followed by non-/ and non-* after a regex-preceding char
        if (
          char === '/' &&
          nextChar !== '/' &&
          nextChar !== '*' &&
          regexPreceders.has(lastSignificantChar)
        ) {
          inRegex = true
          continue
        }
      }

      // Handle string boundaries (skip escaped quotes) - only when not in comment
      if ((char === '"' || char === "'" || char === '`') && prevChar !== '\\') {
        if (inString === null) {
          inString = char // Enter string
        } else if (inString === char) {
          inString = null // Exit string
        }
        // Don't continue - we still need to check parenCount at end of loop
      }

      // Only count parentheses outside of strings, comments, and regex
      if (inString === null) {
        if (char === '(') {
          parenCount++
          foundFirstParen = true
        } else if (char === ')') {
          parenCount--
        }
      }

      // Track last significant (non-whitespace) character for regex detection
      if (inString === null && !/\s/.test(char)) {
        lastSignificantChar = char
      }

      if (foundFirstParen && parenCount === 0) {
        testEnd = i + 1
        break
      }
    }

    const rawContent = content.substring(testStart, testEnd)

    const hasGiven = GIVEN_PATTERN.test(rawContent)
    const hasWhen = WHEN_PATTERN.test(rawContent)
    // THEN can be explicit comment or implicit via expect().rejects/.resolves pattern
    const hasThen = THEN_PATTERN.test(rawContent) || IMPLICIT_THEN_PATTERN.test(rawContent)

    const idMatch = testName.match(SPEC_ID_PATTERN)
    const id = idMatch?.[1] ?? null

    const tagMatch = rawContent.match(TAG_PATTERN)
    const tag = tagMatch ? (tagMatch[1] as '@spec' | '@regression') : null

    tests.push({
      id,
      name: testName as string,
      tag,
      hasGiven,
      hasWhen,
      hasThen,
      lineNumber,
      isFixme: true,
      rawContent,
    })
  }

  return tests.sort((a, b) => a.lineNumber - b.lineNumber)
}

function analyzeQuality(file: SpecFile, content: string): QualityIssue[] {
  const issues: QualityIssue[] = []

  for (const test of file.tests) {
    // Skip regression tests - they are mirrors of @spec tests and don't need individual validation
    // Regression tests are identified by @regression tag OR -REGRESSION suffix in spec ID
    const isRegressionTest =
      test.tag === '@regression' || (test.id !== null && test.id.endsWith('-REGRESSION'))
    if (isRegressionTest) {
      continue
    }

    // Check for spec ID - required for @spec tests
    if (!test.id && test.tag === '@spec') {
      issues.push({
        type: 'error',
        code: 'MISSING_SPEC_ID',
        message: `Test missing spec ID (expected format: APP-FEATURE-001)`,
        line: test.lineNumber,
        testId: test.name,
      })
    }

    // Check for GIVEN/WHEN/THEN structure (skip fixme tests - they're placeholders)
    if (test.tag === '@spec' && !test.isFixme) {
      if (!test.hasGiven) {
        issues.push({
          type: 'warning',
          code: 'MISSING_GIVEN',
          message: `Test missing GIVEN comment (setup context)`,
          line: test.lineNumber,
          testId: test.id || test.name,
        })
      }
      if (!test.hasWhen) {
        issues.push({
          type: 'warning',
          code: 'MISSING_WHEN',
          message: `Test missing WHEN comment (action)`,
          line: test.lineNumber,
          testId: test.id || test.name,
        })
      }
      if (!test.hasThen) {
        issues.push({
          type: 'warning',
          code: 'MISSING_THEN',
          message: `Test missing THEN comment (assertion)`,
          line: test.lineNumber,
          testId: test.id || test.name,
        })
      }
    }

    // Check for tag - tests must have @spec or @regression tag
    if (!test.tag && !test.isFixme) {
      issues.push({
        type: 'error',
        code: 'MISSING_TAG',
        message: `Test missing tag (@spec or @regression)`,
        line: test.lineNumber,
        testId: test.id || test.name,
      })
    }

    // Check test name quality
    if (test.name.length < 20) {
      issues.push({
        type: 'suggestion',
        code: 'SHORT_TEST_NAME',
        message: `Test name is too short - consider being more descriptive`,
        line: test.lineNumber,
        testId: test.id || test.name,
      })
    }

    // Check for vague test names
    const vaguePatterns = [/^should work$/i, /^test\s+\d+$/i, /^it works$/i, /^basic test$/i]
    if (vaguePatterns.some((p) => p.test(test.name))) {
      issues.push({
        type: 'error',
        code: 'VAGUE_TEST_NAME',
        message: `Test name is too vague - describe the specific behavior`,
        line: test.lineNumber,
        testId: test.id || test.name,
      })
    }

    // Check for TODO comments in test
    TODO_PATTERN.lastIndex = 0
    let todoMatch: RegExpExecArray | null
    while ((todoMatch = TODO_PATTERN.exec(test.rawContent)) !== null) {
      const todoText = todoMatch[1]?.trim() || 'No description'
      issues.push({
        type: 'warning',
        code: 'TODO_IN_TEST',
        message: `TODO: ${todoText}`,
        line: test.lineNumber,
        testId: test.id || test.name,
      })
    }
  }

  // Check file-level issues
  const specTests = file.tests.filter((t) => t.tag === '@spec')
  const regressionTests = file.tests.filter((t) => t.tag === '@regression')

  // Check header "Spec Count: X" matches actual @spec count
  const headerLines = content.split('\n').slice(0, 50).join('\n')
  const specCountMatch = headerLines.match(/\*\s*Spec Count:\s*(\d+)/)
  if (specCountMatch?.[1]) {
    const headerCount = parseInt(specCountMatch[1], 10)
    const actualCount = specTests.length

    if (headerCount !== actualCount) {
      issues.push({
        type: 'error',
        code: 'HEADER_COUNT_MISMATCH',
        message: `Header "Spec Count: ${headerCount}" does not match actual @spec count: ${actualCount}`,
      })
    }
  }

  if (specTests.length > 0 && regressionTests.length === 0) {
    issues.push({
      type: 'suggestion',
      code: 'MISSING_REGRESSION',
      message: `File has @spec tests but no @regression test for consolidated workflow`,
    })
  }

  // Check for sequential spec IDs (exclude regression tests - they have -REGRESSION suffix, not numeric)
  const specTestIds = file.tests
    .filter((t) => t.tag === '@spec' && t.id && !t.id.endsWith('-REGRESSION'))
    .map((t) => t.id)
    .filter(Boolean) as string[]
  if (specTestIds.length > 1) {
    const idNumbers = specTestIds.map((id) => {
      const numMatch = id.match(/(\d{3})$/)
      return numMatch?.[1] ? parseInt(numMatch[1], 10) : 0
    })

    for (let i = 1; i < idNumbers.length; i++) {
      const current = idNumbers[i]
      const previous = idNumbers[i - 1]
      if (current !== undefined && previous !== undefined && current !== previous + 1) {
        issues.push({
          type: 'suggestion',
          code: 'NON_SEQUENTIAL_IDS',
          message: `Spec IDs are not sequential (gap between ${specTestIds[i - 1]} and ${specTestIds[i]})`,
        })
        break
      }
    }
  }

  return issues
}

function extractFeatureName(filePath: string): string {
  const relativePath = relative(SPECS_DIR, filePath)
  const parts = relativePath.replace(/\.spec\.ts$/, '').split('/')

  // Remove common prefixes
  if (parts[0] === 'app' || parts[0] === 'api' || parts[0] === 'static') {
    parts.shift()
  }

  return parts.join(' > ') || basename(filePath, '.spec.ts')
}

async function analyzeFile(filePath: string): Promise<SpecFile> {
  const content = await readFile(filePath, 'utf-8')
  const relativePath = relative(SPECS_DIR, filePath)
  const feature = extractFeatureName(filePath)

  // Extract describe label
  const describeMatch = content.match(DESCRIBE_PATTERN)
  const describeLabel = describeMatch?.[1] ?? null

  const tests = extractTests(content, filePath)
  const specTests = tests.filter((t) => t.tag === '@spec')
  const regressionTests = tests.filter((t) => t.tag === '@regression')
  const fixmeTests = tests.filter((t) => t.isFixme)
  const passingTests = tests.filter((t) => !t.isFixme)

  const file: SpecFile = {
    path: filePath,
    relativePath,
    feature,
    tests,
    issues: [],
    metadata: {
      hasDescribe: !!describeLabel,
      describeLabel,
      totalTests: tests.length,
      specTests: specTests.length,
      regressionTests: regressionTests.length,
      fixmeTests: fixmeTests.length,
      passingTests: passingTests.length,
    },
  }

  file.issues = analyzeQuality(file, content)

  return file
}

function calculateQualityScore(files: SpecFile[]): number {
  if (files.length === 0) return 100

  let totalPoints = 0
  let maxPoints = 0

  for (const file of files) {
    // Points for each test
    for (const test of file.tests) {
      maxPoints += 100

      let testPoints = 0

      // Has spec ID: 20 points
      if (test.id) testPoints += 20

      // Has tag: 10 points
      if (test.tag) testPoints += 10

      // Has GIVEN: 20 points
      if (test.hasGiven) testPoints += 20

      // Has WHEN: 20 points
      if (test.hasWhen) testPoints += 20

      // Has THEN: 20 points
      if (test.hasThen) testPoints += 20

      // Not fixme: 10 points
      if (!test.isFixme) testPoints += 10

      totalPoints += testPoints
    }

    // Deduct for issues
    for (const issue of file.issues) {
      if (issue.type === 'error') maxPoints += 10
      else if (issue.type === 'warning') maxPoints += 5
    }
  }

  return maxPoints > 0 ? Math.round((totalPoints / maxPoints) * 100) : 100
}

function detectCoverageGaps(files: SpecFile[]): CoverageGap[] {
  const gaps: CoverageGap[] = []

  // Group files by feature area
  const featureAreas = new Map<string, SpecFile[]>()

  for (const file of files) {
    const area = file.relativePath.split('/')[0] || 'root'
    if (!featureAreas.has(area)) {
      featureAreas.set(area, [])
    }
    featureAreas.get(area)!.push(file)
  }

  // Check for common patterns
  for (const [area, areaFiles] of featureAreas) {
    const allTests = areaFiles.flatMap((f) => f.tests)

    // Skip coverage gap detection for 100% passing domains
    // If all tests pass (no fixme tests), the domain is complete and doesn't need more tests
    const totalTests = allTests.length
    const fixmeTests = allTests.filter((t) => t.isFixme).length
    if (totalTests > 0 && fixmeTests === 0) {
      // Domain is 100% complete - skip coverage gap warnings
      continue
    }

    const testNames = allTests.map((t) => t.name.toLowerCase())

    // Check for missing edge cases
    const hasErrorCases = testNames.some(
      (n) => n.includes('error') || n.includes('invalid') || n.includes('fail')
    )
    const hasEmptyCases = testNames.some(
      (n) => n.includes('empty') || n.includes('missing') || n.includes('undefined')
    )
    const hasBoundaryCases = testNames.some(
      (n) => n.includes('max') || n.includes('min') || n.includes('limit') || n.includes('boundary')
    )

    const missingBehaviors: string[] = []

    if (!hasErrorCases) {
      missingBehaviors.push('Error handling scenarios')
    }
    if (!hasEmptyCases) {
      missingBehaviors.push('Empty/missing data scenarios')
    }
    if (!hasBoundaryCases) {
      missingBehaviors.push('Boundary/edge cases')
    }

    if (missingBehaviors.length > 0) {
      gaps.push({
        feature: area,
        missingBehaviors,
        suggestion: `Consider adding tests for: ${missingBehaviors.join(', ')}`,
      })
    }
  }

  return gaps
}

/**
 * Detect duplicate spec IDs across all files.
 * Each spec ID should be unique across the entire test suite.
 */
function detectDuplicateSpecIds(files: SpecFile[]): DuplicateSpecId[] {
  const specIdMap = new Map<string, Array<{ file: string; line: number; testName: string }>>()

  // Collect all spec IDs with their locations
  for (const file of files) {
    for (const test of file.tests) {
      if (test.id) {
        if (!specIdMap.has(test.id)) {
          specIdMap.set(test.id, [])
        }
        specIdMap.get(test.id)!.push({
          file: file.relativePath,
          line: test.lineNumber,
          testName: test.name,
        })
      }
    }
  }

  // Find duplicates (spec IDs that appear more than once)
  const duplicates: DuplicateSpecId[] = []

  for (const [specId, locations] of specIdMap) {
    if (locations.length > 1) {
      duplicates.push({ specId, locations })
    }
  }

  return duplicates.sort((a, b) => a.specId.localeCompare(b.specId))
}

// =============================================================================
// User Story Analysis Functions
// =============================================================================

/**
 * Find all user story markdown files in docs/user-stories/
 */
async function findUserStoryFiles(dir: string): Promise<string[]> {
  const files: string[] = []

  async function walk(currentDir: string) {
    try {
      const entries = await readdir(currentDir, { withFileTypes: true })

      for (const entry of entries) {
        const fullPath = join(currentDir, entry.name)

        if (entry.isDirectory()) {
          await walk(fullPath)
        } else if (entry.name.endsWith('.md') && entry.name !== 'README.md') {
          // Parse all markdown files except README.md
          files.push(fullPath)
        }
      }
    } catch {
      // Directory doesn't exist or can't be read
    }
  }

  await walk(dir)
  return files.sort()
}

/**
 * Parse a user story markdown file and extract all user stories with their acceptance criteria.
 */
function parseUserStoryFile(content: string, filePath: string): UserStory[] {
  const stories: UserStory[] = []

  // Extract domain, feature area, and role from file metadata
  const domainMatch = content.match(DOMAIN_PATTERN)
  const featureAreaMatch = content.match(FEATURE_AREA_PATTERN)
  const roleMatch = content.match(ROLE_PATTERN)

  const domain = domainMatch?.[1] || 'unknown'
  const featureArea = featureAreaMatch?.[1] || 'unknown'
  const role = roleMatch?.[1]?.trim() || 'unknown'

  // Split content by user story headers (## or ### depending on format)
  const storyBlocks = content
    .split(/(?=^#{2,3}\s+US-)/m)
    .filter((block) => /^#{2,3}\s+US-/.test(block))

  for (const block of storyBlocks) {
    // Extract user story ID and title (## US-AUTH-EMAIL-001: Title or ### US-...)
    const idMatch = block.match(/^#{2,3}\s+(US-[A-Z]+-[A-Z0-9-]+-\d{3}):\s+(.+)$/m)
    if (!idMatch) continue

    const storyId = idMatch[1]
    const title = idMatch[2]?.trim() || ''

    // Extract story text
    const storyMatch = block.match(/\*\*Story\*\*:\s*(.+)$/m)
    const story = storyMatch?.[1]?.trim() || ''

    // Extract status
    const statusMatch = block.match(USER_STORY_STATUS_PATTERN)
    let status: 'complete' | 'partial' | 'not-started' = 'not-started'
    if (statusMatch) {
      const statusChar = statusMatch[1]
      if (statusChar === 'x') status = 'complete'
      else if (statusChar === '~') status = 'partial'
    }

    // Extract acceptance criteria from table
    const acceptanceCriteria: AcceptanceCriterion[] = []

    // Reset regex state
    ACCEPTANCE_CRITERIA_TABLE_ROW_PATTERN.lastIndex = 0

    let acMatch: RegExpExecArray | null
    while ((acMatch = ACCEPTANCE_CRITERIA_TABLE_ROW_PATTERN.exec(block)) !== null) {
      const acId = acMatch[1]
      const criterion = acMatch[2]?.trim() || ''
      const specTestId = acMatch[3] || null // May be empty or missing

      // Status is determined by whether spec test exists and passes
      // For now, mark as complete if spec ID is linked
      const acStatus: 'complete' | 'partial' | 'not-started' = specTestId
        ? 'complete'
        : 'not-started'

      if (acId) {
        acceptanceCriteria.push({
          id: acId,
          criterion,
          specTestId,
          schema: '', // Schema column removed in new format
          status: acStatus,
        })
      }
    }

    if (storyId) {
      stories.push({
        id: storyId,
        title,
        story,
        status,
        acceptanceCriteria,
        filePath: relative(process.cwd(), filePath),
        domain,
        featureArea,
        role,
      })
    }
  }

  return stories
}

/**
 * Parse all user story files and return metrics.
 */
async function parseUserStoryFiles(
  files: SpecFile[],
  fixmeSpecIds: Set<string>,
  passingSpecIds: Set<string>
): Promise<UserStoryMetrics> {
  const userStoryFiles = await findUserStoryFiles(USER_STORIES_DIR)
  const allStories: UserStory[] = []

  for (const filePath of userStoryFiles) {
    const content = await readFile(filePath, 'utf-8')
    const stories = parseUserStoryFile(content, filePath)
    allStories.push(...stories)
  }

  // Categorize stories based on their spec IDs
  const storiesToSpecify: UserStory[] = []
  const storiesSpecifiedNotImplemented: UserStory[] = []
  const storiesSpecifiedAndImplemented: UserStory[] = []

  for (const story of allStories) {
    // Get all spec IDs linked to this story's acceptance criteria
    const linkedSpecIds = story.acceptanceCriteria
      .map((ac) => ac.specTestId)
      .filter((id): id is string => id !== null)

    if (linkedSpecIds.length === 0) {
      // No spec IDs linked - needs specification
      storiesToSpecify.push(story)
    } else {
      // Check if ALL linked spec IDs are passing or if some are still fixme
      const allImplemented = linkedSpecIds.every((id) => passingSpecIds.has(id))
      const someFixme = linkedSpecIds.some((id) => fixmeSpecIds.has(id))

      if (allImplemented) {
        storiesSpecifiedAndImplemented.push(story)
      } else if (someFixme) {
        storiesSpecifiedNotImplemented.push(story)
      } else {
        // Spec IDs linked but not found in codebase - treat as needs specification
        storiesToSpecify.push(story)
      }
    }
  }

  // Calculate percentages
  const total = allStories.length || 1 // Avoid division by zero
  const storiesToSpecifyPercent = Math.round((storiesToSpecify.length / total) * 100)
  const storiesSpecifiedNotImplementedPercent = Math.round(
    (storiesSpecifiedNotImplemented.length / total) * 100
  )
  const storiesSpecifiedAndImplementedPercent = Math.round(
    (storiesSpecifiedAndImplemented.length / total) * 100
  )

  // Total criteria count
  const totalCriteria = allStories.reduce((sum, s) => sum + s.acceptanceCriteria.length, 0)

  // Organize by domain
  const byDomain = new Map<string, UserStory[]>()
  for (const story of allStories) {
    const domain = story.domain.toUpperCase()
    if (!byDomain.has(domain)) {
      byDomain.set(domain, [])
    }
    byDomain.get(domain)!.push(story)
  }

  return {
    totalStories: allStories.length,
    totalCriteria,
    storiesToSpecify,
    storiesToSpecifyPercent,
    storiesSpecifiedNotImplemented,
    storiesSpecifiedNotImplementedPercent,
    storiesSpecifiedAndImplemented,
    storiesSpecifiedAndImplementedPercent,
    byDomain,
  }
}

/**
 * Update user story files with test status column in acceptance criteria tables.
 *
 * Status values:
 * - ‚úÖ = Test is passing (spec ID found in passingSpecIds)
 * - ‚è≥ = Test is .fixme() (spec ID found in fixmeSpecIds)
 * - ‚ùì = Spec ID not found in codebase
 * - (empty) = No spec ID linked
 */
async function updateUserStoryFiles(
  fixmeSpecIds: Set<string>,
  passingSpecIds: Set<string>
): Promise<{ updated: number; unchanged: number; changes: string[] }> {
  const userStoryFiles = await findUserStoryFiles(USER_STORIES_DIR)
  let updated = 0
  let unchanged = 0
  const changes: string[] = []

  for (const filePath of userStoryFiles) {
    const content = await readFile(filePath, 'utf-8')
    const updatedContent = updateAcceptanceCriteriaTables(content, fixmeSpecIds, passingSpecIds)

    // Format with Prettier before comparing
    const prettierConfig = await prettier.resolveConfig(filePath)
    const formattedContent = await prettier.format(updatedContent, {
      ...prettierConfig,
      parser: 'markdown',
    })

    // Compare formatted content with original to detect actual changes
    if (formattedContent !== content) {
      await writeFile(filePath, formattedContent)
      updated++
      changes.push(relative(process.cwd(), filePath))
    } else {
      unchanged++
    }
  }

  return { updated, unchanged, changes }
}

/**
 * Update acceptance criteria tables in markdown content to add/update Status column.
 *
 * Handles tables with format:
 * | ID | Criterion | E2E Spec |
 * | -- | --------- | -------- |
 * | AC-001 | Criterion text | `SPEC-ID-001` |
 *
 * Transforms to:
 * | ID | Criterion | E2E Spec | Status |
 * | -- | --------- | -------- | ------ |
 * | AC-001 | Criterion text | `SPEC-ID-001` | ‚úÖ |
 */
function updateAcceptanceCriteriaTables(
  content: string,
  fixmeSpecIds: Set<string>,
  passingSpecIds: Set<string>
): string {
  // Match acceptance criteria tables (header row + separator row + data rows)
  // Header pattern: | ID | Criterion | E2E Spec | (optional Status column)
  const tablePattern =
    /(\|\s*ID\s*\|\s*Criterion\s*\|\s*E2E Spec\s*)(\|\s*Status\s*)?(\s*\|?\s*\n)(\|\s*[-:]+\s*\|\s*[-:]+\s*\|\s*[-:]+\s*)(\|\s*[-:]+\s*)?(\s*\|?\s*\n)((?:\|[^\n]+\|\s*\n?)+)/gi

  return content.replace(
    tablePattern,
    (
      _match,
      headerBase: string,
      _existingStatusHeader: string | undefined,
      headerEnd: string,
      separatorBase: string,
      _existingStatusSeparator: string | undefined,
      separatorEnd: string,
      dataRows: string
    ) => {
      // Build new header with Status column
      const newHeader = `${headerBase.trimEnd()}| Status ${headerEnd}`

      // Build new separator with Status column
      const newSeparator = `${separatorBase.trimEnd()}| ------ ${separatorEnd}`

      // Process each data row
      const newDataRows = dataRows
        .split('\n')
        .filter((row) => row.trim())
        .map((row) => {
          // Parse the row to extract spec ID
          // Format: | AC-001 | Criterion text | `SPEC-ID-001` | (optional existing status)
          const rowMatch = row.match(
            /^\|\s*(AC-\d{3})\s*\|\s*([^|]+)\s*\|\s*`?([A-Z]+-[A-Z0-9-]+-(?:\d{3}|REGRESSION))?`?\s*(?:\|\s*[^|]*)?(\s*\|?\s*)$/
          )

          if (!rowMatch) {
            // If row doesn't match expected format, return as-is but try to add empty status
            if (row.match(/^\|.*\|.*\|.*\|$/)) {
              // Row has 3 columns, add status column
              return row.replace(/\|(\s*)$/, '|  |')
            }
            return row
          }

          const acId = rowMatch[1]
          const criterion = rowMatch[2]?.trim() || ''
          const specId = rowMatch[3] || ''

          // Determine status based on spec ID
          let status = ''
          if (!specId) {
            status = '' // No spec ID linked - leave empty
          } else if (passingSpecIds.has(specId)) {
            status = '‚úÖ'
          } else if (fixmeSpecIds.has(specId)) {
            status = '‚è≥'
          } else {
            status = '‚ùì' // Spec ID exists but not found in codebase
          }

          // Rebuild the row with status column
          // Preserve original formatting as much as possible
          const specIdFormatted = specId ? `\`${specId}\`` : ''
          return `| ${acId} | ${criterion} | ${specIdFormatted} | ${status} |`
        })
        .join('\n')

      return `${newHeader}${newSeparator}${newDataRows}\n`
    }
  )
}

/**
 * Detect spec IDs in E2E tests that are NOT linked to any user story.
 * These are "orphaned" specs that exist without documented requirements.
 */
function detectOrphanedSpecIds(
  files: SpecFile[],
  userStoryMetrics: UserStoryMetrics
): OrphanedSpecId[] {
  // Collect all spec IDs referenced in user stories
  const linkedSpecIds = new Set<string>()

  for (const stories of userStoryMetrics.byDomain.values()) {
    for (const story of stories) {
      for (const ac of story.acceptanceCriteria) {
        if (ac.specTestId) {
          linkedSpecIds.add(ac.specTestId)
        }
      }
    }
  }

  // Find spec IDs in E2E tests that are not in user stories
  const orphaned: OrphanedSpecId[] = []

  for (const file of files) {
    for (const test of file.tests) {
      if (test.id && !linkedSpecIds.has(test.id)) {
        orphaned.push({
          specId: test.id,
          file: file.relativePath,
          line: test.lineNumber,
          testName: test.name,
        })
      }
    }
  }

  return orphaned.sort((a, b) => a.specId.localeCompare(b.specId))
}

/**
 * Calculate TDD automation statistics from git history.
 * Looks for commits with "fix: implement" pattern (TDD workflow commits).
 */
async function calculateTDDAutomationStats(totalFixme: number): Promise<TDDAutomationStats> {
  const { exec } = await import('node:child_process')
  const { promisify } = await import('node:util')
  const execAsync = promisify(exec)

  // Pattern to match any commit containing a spec ID (e.g., APP-XXX-001)
  // Matches: "fix: implement APP-XXX-001", "test: activate APP-XXX-001", "feat: implement APP-XXX-001", etc.
  // Also matches TDD pipeline format: "[TDD] Implement API-TABLES-RECORDS-LIST-009 | Attempt 5/5 (#7054)"
  const specIdPattern = /([A-Z]+-[A-Z0-9-]+-\d{3})/

  try {
    // Get all spec-related commits in the last 90 days
    // Uses multiple --grep flags (implicit OR) to match:
    // 1. Conventional commits: fix:, test:, feat:, refactor:, chore: with spec IDs
    // 2. TDD pipeline commits: [TDD] Implement <SPEC-ID> | Attempt X/N (#PR)
    // 3. TDD activation commits: test(tdd): activate <SPEC-ID>
    const { stdout } = await execAsync(
      'git log --oneline --since="90 days ago" --extended-regexp' +
        ' --grep="^(fix|test|feat|refactor|chore)(\\([^)]*\\))?:.*[A-Z]+-[A-Z0-9-]+-([0-9]{3}|REGRESSION)"' +
        ' --grep="^\\[TDD\\].*[A-Z]+-[A-Z0-9-]+-([0-9]{3}|REGRESSION)"' +
        ' --grep="^test\\(tdd\\):.*[A-Z]+-[A-Z0-9-]+-([0-9]{3}|REGRESSION)"' +
        ' --format="%H|%aI|%s"',
      { cwd: process.cwd(), maxBuffer: 10 * 1024 * 1024 }
    )

    const commits = stdout
      .trim()
      .split('\n')
      .filter(Boolean)
      .map((line) => {
        const [hash, date, ...messageParts] = line.split('|')
        const message = messageParts.join('|')
        const match = message?.match(specIdPattern)
        // Detect source: TDD pipeline commits start with "[TDD]"
        const source: 'tdd-pipeline' | 'manual' = message?.startsWith('[TDD]')
          ? 'tdd-pipeline'
          : 'manual'
        return {
          hash: hash || '',
          date: date || '',
          specId: match?.[1] || null,
          source,
        }
      })
      .filter((c) => c.specId !== null) as Array<{
      hash: string
      date: string
      specId: string
      source: 'tdd-pipeline' | 'manual'
    }>

    // Calculate time-based metrics
    const now = new Date()
    const oneDayAgo = new Date(now.getTime() - 24 * 60 * 60 * 1000)
    const sevenDaysAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000)
    const thirtyDaysAgo = new Date(now.getTime() - 30 * 24 * 60 * 60 * 1000)

    const fixedLast24h = commits.filter((c) => new Date(c.date) >= oneDayAgo).length
    const fixedLast7d = commits.filter((c) => new Date(c.date) >= sevenDaysAgo).length
    const fixedLast30d = commits.filter((c) => new Date(c.date) >= thirtyDaysAgo).length

    // Calculate average fixes per day (based on 30-day window for stability)
    const avgFixesPerDay = fixedLast30d / 30

    // Estimate remaining time
    let estimatedDaysRemaining: number | null = null
    let estimatedCompletionDate: string | null = null

    if (avgFixesPerDay > 0 && totalFixme > 0) {
      estimatedDaysRemaining = Math.ceil(totalFixme / avgFixesPerDay)
      const completionDate = new Date(now.getTime() + estimatedDaysRemaining * 24 * 60 * 60 * 1000)
      estimatedCompletionDate = completionDate.toISOString().split('T')[0] || null
    }

    // Get recent fixes (last 20)
    const recentFixes = commits.slice(0, 20).map((c) => ({
      specId: c.specId,
      date: c.date.replace('T', ' ').substring(0, 16),
      commitHash: c.hash.substring(0, 7),
      source: c.source,
    }))

    // Count by source
    const fixedByPipeline = commits.filter((c) => c.source === 'tdd-pipeline').length
    const fixedManually = commits.filter((c) => c.source === 'manual').length

    return {
      totalFixed: commits.length,
      fixedByPipeline,
      fixedManually,
      fixedLast24h,
      fixedLast7d,
      fixedLast30d,
      avgFixesPerDay: Math.round(avgFixesPerDay * 100) / 100,
      estimatedDaysRemaining,
      estimatedCompletionDate,
      recentFixes,
    }
  } catch {
    // If git command fails, return empty stats
    return {
      totalFixed: 0,
      fixedByPipeline: 0,
      fixedManually: 0,
      fixedLast24h: 0,
      fixedLast7d: 0,
      fixedLast30d: 0,
      avgFixesPerDay: 0,
      estimatedDaysRemaining: null,
      estimatedCompletionDate: null,
      recentFixes: [],
    }
  }
}

/**
 * Verify progress by cross-referencing GitHub PRs, issues, and commits.
 * Detects discrepancies between what's been fixed and what's still marked as fixme.
 */
async function verifyProgressFromGitHub(
  fixmeSpecIds: Set<string>,
  passingSpecIds: Set<string>
): Promise<VerificationResult> {
  const { exec } = await import('node:child_process')
  const { promisify } = await import('node:util')
  const execAsync = promisify(exec)

  const specIdPattern = /([A-Z]+-[A-Z0-9-]+-\d{3})/g
  const allReferences: GitHubReference[] = []
  const fixedSpecIds = new Set<string>()

  try {
    // 1. Fetch merged PRs with spec IDs (last 90 days)
    const { stdout: prOutput } = await execAsync(
      'gh pr list --state merged --limit 500 --json number,title,mergedAt,url',
      { cwd: process.cwd(), maxBuffer: 10 * 1024 * 1024 }
    )

    const prs = JSON.parse(prOutput) as Array<{
      number: number
      title: string
      mergedAt: string
      url: string
    }>

    for (const pr of prs) {
      const matches = pr.title.matchAll(specIdPattern)
      for (const match of matches) {
        const specId = match[1]
        if (!specId) continue
        allReferences.push({
          type: 'pr',
          number: pr.number,
          title: pr.title,
          specId,
          date: pr.mergedAt,
          url: pr.url,
        })
        fixedSpecIds.add(specId)
      }
    }

    // 2. Fetch closed issues with tdd-spec:completed label
    const { stdout: issueOutput } = await execAsync(
      'gh issue list --state closed --label "tdd-spec:completed" --limit 500 --json number,title,closedAt,url',
      { cwd: process.cwd(), maxBuffer: 10 * 1024 * 1024 }
    )

    const issues = JSON.parse(issueOutput) as Array<{
      number: number
      title: string
      closedAt: string
      url: string
    }>

    for (const issue of issues) {
      const matches = issue.title.matchAll(specIdPattern)
      for (const match of matches) {
        const specId = match[1]
        if (!specId) continue
        allReferences.push({
          type: 'issue',
          number: issue.number,
          title: issue.title,
          specId,
          date: issue.closedAt,
          url: issue.url,
        })
        fixedSpecIds.add(specId)
      }
    }

    // 3. Fetch commits with spec IDs (last 90 days) - all conventional commit types
    const { stdout: commitOutput } = await execAsync(
      'git log --oneline --since="90 days ago" --extended-regexp --grep="[A-Z]+-[A-Z0-9-]+-[0-9]{3}" --format="%H|%aI|%s"',
      { cwd: process.cwd(), maxBuffer: 10 * 1024 * 1024 }
    )

    const commitLines = commitOutput.trim().split('\n').filter(Boolean)
    for (const line of commitLines) {
      const [hash, date, ...messageParts] = line.split('|')
      const message = messageParts.join('|')
      if (!hash || !date || !message) continue

      const matches = message.matchAll(specIdPattern)
      for (const match of matches) {
        const specId = match[1]
        if (!specId) continue
        allReferences.push({
          type: 'commit',
          hash: hash.substring(0, 7),
          title: message,
          specId,
          date,
        })
        fixedSpecIds.add(specId)
      }
    }

    // 4. Detect discrepancies
    const discrepancies: ProgressDiscrepancy[] = []

    // Find specs that have been fixed (in PRs/commits) but are still marked as fixme
    for (const specId of fixedSpecIds) {
      if (fixmeSpecIds.has(specId)) {
        const refs = allReferences.filter((r) => r.specId === specId)
        discrepancies.push({
          specId,
          status: 'still-fixme',
          references: refs,
          suggestion: `Spec ${specId} has been fixed (see PRs/commits) but is still marked as .fixme() in the test file`,
        })
      }
    }

    // Find specs that are passing but not tracked in TDD automation
    for (const specId of passingSpecIds) {
      if (!fixedSpecIds.has(specId)) {
        // This is normal for specs that were never fixme - skip
        // Only flag if we expected it to be tracked
      }
    }

    return {
      allReferences,
      discrepancies,
      fixedSpecIds,
    }
  } catch (error) {
    console.warn('Warning: Could not verify progress from GitHub:', error)
    return {
      allReferences: [],
      discrepancies: [],
      fixedSpecIds: new Set(),
    }
  }
}

// =============================================================================
// Output Generation
// =============================================================================

/**
 * Calculate domain-level summaries for the Table of Contents and rollup display.
 */
function calculateDomainSummaries(files: SpecFile[]): DomainSummary[] {
  const byDomain = new Map<string, SpecFile[]>()

  for (const file of files) {
    const domain = file.relativePath.split('/')[0] || 'root'
    if (!byDomain.has(domain)) {
      byDomain.set(domain, [])
    }
    byDomain.get(domain)!.push(file)
  }

  const summaries: DomainSummary[] = []

  for (const [domain, domainFiles] of byDomain) {
    const totalTests = domainFiles.reduce((sum, f) => sum + f.metadata.totalTests, 0)
    const passingTests = domainFiles.reduce((sum, f) => sum + f.metadata.passingTests, 0)
    const fixmeTests = domainFiles.reduce((sum, f) => sum + f.metadata.fixmeTests, 0)
    const progressPercent = totalTests > 0 ? Math.round((passingTests / totalTests) * 100) : 100

    // Determine status based on progress and issues
    const hasErrors = domainFiles.some((f) => f.issues.some((i) => i.type === 'error'))
    const hasWarnings = domainFiles.some((f) => f.issues.some((i) => i.type === 'warning'))

    let status: 'üü¢' | 'üü°' | 'üî¥'
    if (hasErrors) {
      status = 'üî¥'
    } else if (progressPercent === 100 && !hasWarnings) {
      status = 'üü¢'
    } else if (progressPercent >= 80 || (progressPercent === 100 && hasWarnings)) {
      status = 'üü°'
    } else {
      status = 'üü°'
    }

    summaries.push({
      domain: domain.toUpperCase(),
      totalFiles: domainFiles.length,
      totalTests,
      passingTests,
      fixmeTests,
      progressPercent,
      status,
      files: domainFiles,
    })
  }

  // Sort by domain name for consistent ordering
  return summaries.sort((a, b) => a.domain.localeCompare(b.domain))
}

/**
 * Get a human-readable status label based on progress.
 */
function getStatusLabel(progressPercent: number, hasIssues: boolean): string {
  if (progressPercent === 100 && !hasIssues) return 'Complete'
  if (progressPercent === 100 && hasIssues) return 'Review Needed'
  if (progressPercent >= 80) return 'Almost Done'
  if (progressPercent >= 50) return 'In Progress'
  return 'Early Stage'
}

/**
 * Get overall health status for executive summary.
 */
function getOverallHealth(
  progressPercent: number,
  errors: number,
  avgFixesPerDay: number
): { emoji: string; label: string } {
  if (errors > 0) return { emoji: 'üî¥', label: 'Blocked' }
  if (avgFixesPerDay >= 25 && progressPercent >= 50) return { emoji: 'üü¢', label: 'On Track' }
  if (avgFixesPerDay >= 15) return { emoji: 'üü°', label: 'Progressing' }
  return { emoji: 'üü°', label: 'Needs Attention' }
}

function generateMarkdown(state: SpecState): string {
  const lines: string[] = []

  // Calculate domain summaries for TOC and rollups
  const domainSummaries = calculateDomainSummaries(state.files)

  // Calculate key metrics
  const progressPercent = Math.round(
    (state.summary.totalPassing / Math.max(state.summary.totalTests, 1)) * 100
  )
  const progressFilled = Math.round(progressPercent / 5)
  const progressEmpty = 20 - progressFilled

  // Get overall health status
  const health = getOverallHealth(
    progressPercent,
    state.summary.issuesByType.errors,
    state.tddAutomation.avgFixesPerDay
  )

  // Calculate ETA text
  let etaText = 'Unknown'
  if (state.tddAutomation.estimatedDaysRemaining !== null) {
    const days = state.tddAutomation.estimatedDaysRemaining
    etaText =
      days === 0
        ? 'Less than 1 day'
        : days === 1
          ? '~1 day'
          : days < 7
            ? `~${days} days`
            : days < 30
              ? `~${Math.ceil(days / 7)} weeks`
              : `~${Math.ceil(days / 30)} months`
  }

  // ==========================================================================
  // HEADER
  // ==========================================================================
  lines.push('# Spec Progress Report')
  lines.push('')
  lines.push(`> Generated: ${state.generatedAt}`)
  lines.push('>')
  lines.push('> Auto-generated by `bun run progress` ‚Ä¢ Track test coverage and TDD progress')
  lines.push('')

  // ==========================================================================
  // EXECUTIVE SUMMARY (consolidated table - no prose duplication)
  // ==========================================================================
  lines.push('## üéØ Executive Summary')
  lines.push('')
  lines.push('| Category | Metric | Status |')
  lines.push('|----------|--------|--------|')
  lines.push(
    `| **Overall Progress** | [${'‚ñà'.repeat(progressFilled)}${'‚ñë'.repeat(progressEmpty)}] ${progressPercent}% (${state.summary.totalPassing}/${state.summary.totalTests} tests) | ${health.emoji} ${health.label} |`
  )
  lines.push(
    `| **Daily Velocity** | ${state.tddAutomation.avgFixesPerDay} specs/day | ${state.tddAutomation.avgFixesPerDay >= 25 ? 'üü¢ Healthy' : state.tddAutomation.avgFixesPerDay >= 15 ? 'üü° Moderate' : 'üî¥ Slow'} |`
  )
  lines.push(
    `| **Quality Score** | ${state.summary.qualityScore}% | ${state.summary.qualityScore >= 90 ? 'üü¢ Excellent' : state.summary.qualityScore >= 75 ? 'üü° Good' : 'üî¥ Needs Work'} |`
  )
  if (state.tddAutomation.estimatedCompletionDate) {
    lines.push(
      `| **Estimated Completion** | ${state.tddAutomation.estimatedCompletionDate} (${etaText}) | ${health.emoji} ${health.label} |`
    )
  }
  lines.push(
    `| **Blockers** | ${state.summary.issuesByType.errors} errors, ${state.summary.issuesByType.warnings} warnings | ${state.summary.issuesByType.errors === 0 ? 'üü¢ Clear' : 'üî¥ Blocked'} |`
  )
  lines.push('')

  // ==========================================================================
  // TABLE OF CONTENTS
  // ==========================================================================
  lines.push('## üìë Table of Contents')
  lines.push('')
  lines.push('- [Executive Summary](#-executive-summary)')
  lines.push('- [Next Steps](#-next-steps)')
  lines.push('- [Detailed Metrics](#-detailed-metrics)')
  lines.push('- [User Stories Metrics](#-user-stories-metrics)')
  lines.push('- [TDD Automation](#-tdd-automation)')
  lines.push('- [Feature Breakdown](#-feature-breakdown)')
  for (const domain of domainSummaries) {
    lines.push(
      `  - [${domain.domain}](#${domain.domain.toLowerCase()}) ‚Ä¢ ${domain.status} ${domain.progressPercent}% (${domain.totalFiles} files)`
    )
  }
  lines.push('- [Quick Reference](#-quick-reference)')
  lines.push('')

  // ==========================================================================
  // NEXT STEPS (Actionable)
  // ==========================================================================
  lines.push('## üéØ Next Steps')
  lines.push('')

  // Collect all issues
  const allIssues = state.files.flatMap((f) =>
    f.issues.map((i) => ({ ...i, file: f.relativePath }))
  )
  const errors = allIssues.filter((i) => i.type === 'error')
  const warnings = allIssues.filter((i) => i.type === 'warning')

  // Priority 1 - Immediate Action
  if (errors.length > 0 || state.duplicateSpecIds.length > 0) {
    lines.push('### üö® Priority 1 - Immediate Action')
    lines.push('')
    if (state.duplicateSpecIds.length > 0) {
      lines.push(
        `- **Fix ${state.duplicateSpecIds.length} duplicate spec IDs** - Critical for TDD queue`
      )
      for (const dup of state.duplicateSpecIds.slice(0, 3)) {
        lines.push(`  - \`${dup.specId}\` appears ${dup.locations.length} times`)
      }
    }
    for (const error of errors.slice(0, 5)) {
      const location = error.line ? `:${error.line}` : ''
      lines.push(`- ‚ùå **${error.code}** in \`${error.file}${location}\``)
    }
    lines.push('')
  }

  // Priority 2 - This Week
  if (warnings.length > 0 || state.coverageGaps.length > 0) {
    lines.push('### ‚ö†Ô∏è Priority 2 - This Week')
    lines.push('')
    for (const warning of warnings.slice(0, 5)) {
      const location = warning.line ? `:${warning.line}` : ''
      lines.push(`- **Fix warning** in \`${warning.file}${location}\` (${warning.code})`)
    }
    if (state.coverageGaps.length > 0) {
      lines.push(
        `- **Review ${state.coverageGaps.length} coverage gaps** - Add missing test scenarios`
      )
    }
    lines.push('')
  }

  // Priority 3 - Ongoing
  lines.push('### üìã Priority 3 - Ongoing')
  lines.push('')
  const requiredVelocity =
    state.tddAutomation.estimatedDaysRemaining !== null &&
    state.tddAutomation.estimatedDaysRemaining > 0
      ? Math.ceil(state.summary.totalFixme / state.tddAutomation.estimatedDaysRemaining)
      : state.tddAutomation.avgFixesPerDay
  lines.push(
    `- **Maintain velocity** - Need ${requiredVelocity} specs/day to hit ETA (current: ${state.tddAutomation.avgFixesPerDay})`
  )
  lines.push(`- **${state.summary.totalFixme} tests remaining** - TDD automation running`)
  lines.push(
    `- **Quality gate** - Keep quality score above 85% (current: ${state.summary.qualityScore}%)`
  )
  lines.push('')

  // ==========================================================================
  // DETAILED METRICS
  // ==========================================================================
  lines.push('## üìä Detailed Metrics')
  lines.push('')
  lines.push('| Metric | Value |')
  lines.push('|--------|-------|')
  lines.push(`| **Quality Score** | ${state.summary.qualityScore}% |`)
  lines.push(`| Total Files | ${state.summary.totalFiles} |`)
  lines.push(`| Total Tests | ${state.summary.totalTests} |`)
  lines.push(`| @spec Tests | ${state.summary.totalSpecs} |`)
  lines.push(`| @regression Tests | ${state.summary.totalRegressions} |`)
  lines.push(`| ‚úÖ Passing | ${state.summary.totalPassing} |`)
  lines.push(`| ‚è∏Ô∏è Fixme | ${state.summary.totalFixme} |`)
  lines.push(`| ‚ùå Errors | ${state.summary.issuesByType.errors} |`)
  lines.push(`| ‚ö†Ô∏è Warnings | ${state.summary.issuesByType.warnings} |`)
  lines.push(`| üí° Suggestions | ${state.summary.issuesByType.suggestions} |`)
  lines.push(`| üîÑ Duplicate IDs | ${state.summary.duplicateSpecIds} |`)
  lines.push(`| üîó Orphaned Spec IDs | ${state.summary.orphanedSpecIds} |`)
  lines.push('')

  // Progress bar removed - now only in Executive Summary to avoid redundancy

  // ==========================================================================
  // USER STORIES METRICS
  // ==========================================================================
  if (state.userStoryMetrics && state.userStoryMetrics.totalStories > 0) {
    const metrics = state.userStoryMetrics

    lines.push('## üìã User Stories Metrics')
    lines.push('')
    lines.push('| Category | Count | Percentage |')
    lines.push('|----------|-------|------------|')
    lines.push(`| **Total User Stories** | ${metrics.totalStories} | - |`)
    lines.push(`| **Total Acceptance Criteria** | ${metrics.totalCriteria} | - |`)
    lines.push(
      `| üü¢ Specified & Implemented | ${metrics.storiesSpecifiedAndImplemented.length} | ${metrics.storiesSpecifiedAndImplementedPercent}% |`
    )
    lines.push(
      `| üü° Specified, Not Implemented | ${metrics.storiesSpecifiedNotImplemented.length} | ${metrics.storiesSpecifiedNotImplementedPercent}% |`
    )
    lines.push(
      `| üî¥ Needs Specification | ${metrics.storiesToSpecify.length} | ${metrics.storiesToSpecifyPercent}% |`
    )
    lines.push('')

    // Progress bar for user story coverage
    const implementedPercent = metrics.storiesSpecifiedAndImplementedPercent
    const implementedFilled = Math.round(implementedPercent / 5)
    const implementedEmpty = 20 - implementedFilled
    lines.push(
      `**User Story Coverage**: [${'‚ñà'.repeat(implementedFilled)}${'‚ñë'.repeat(implementedEmpty)}] ${implementedPercent}%`
    )
    lines.push('')

    // Breakdown by domain
    if (metrics.byDomain.size > 0) {
      lines.push('### By Domain')
      lines.push('')
      lines.push('| Domain | Stories | Specified | Implemented | Coverage |')
      lines.push('|--------|---------|-----------|-------------|----------|')

      for (const [domain, stories] of metrics.byDomain) {
        const specified = stories.filter((s) =>
          s.acceptanceCriteria.some((ac) => ac.specTestId !== null)
        ).length
        const implemented = stories.filter((s) =>
          metrics.storiesSpecifiedAndImplemented.includes(s)
        ).length
        const coveragePercent =
          stories.length > 0 ? Math.round((implemented / stories.length) * 100) : 0
        lines.push(
          `| ${domain} | ${stories.length} | ${specified} | ${implemented} | ${coveragePercent}% |`
        )
      }
      lines.push('')
    }

    // Stories needing specification
    if (metrics.storiesToSpecify.length > 0) {
      lines.push('<details>')
      lines.push(
        `<summary>üî¥ Stories Needing Specification (${metrics.storiesToSpecify.length})</summary>`
      )
      lines.push('')
      for (const story of metrics.storiesToSpecify.slice(0, 20)) {
        lines.push(`- **${story.id}**: ${story.title}`)
        lines.push(`  - File: \`${story.filePath}\``)
        lines.push(`  - Domain: ${story.domain} > ${story.featureArea} > ${story.role}`)
      }
      if (metrics.storiesToSpecify.length > 20) {
        lines.push(`- ... and ${metrics.storiesToSpecify.length - 20} more`)
      }
      lines.push('')
      lines.push('</details>')
      lines.push('')
    }

    // Stories specified but not implemented
    if (metrics.storiesSpecifiedNotImplemented.length > 0) {
      lines.push('<details>')
      lines.push(
        `<summary>üü° Stories Specified but Not Implemented (${metrics.storiesSpecifiedNotImplemented.length})</summary>`
      )
      lines.push('')
      for (const story of metrics.storiesSpecifiedNotImplemented.slice(0, 20)) {
        const specIds = story.acceptanceCriteria
          .map((ac) => ac.specTestId)
          .filter((id): id is string => id !== null)
        lines.push(`- **${story.id}**: ${story.title}`)
        lines.push(`  - Spec IDs: ${specIds.map((id) => `\`${id}\``).join(', ')}`)
        lines.push(`  - File: \`${story.filePath}\``)
      }
      if (metrics.storiesSpecifiedNotImplemented.length > 20) {
        lines.push(`- ... and ${metrics.storiesSpecifiedNotImplemented.length - 20} more`)
      }
      lines.push('')
      lines.push('</details>')
      lines.push('')
    }
  }

  // ==========================================================================
  // ORPHANED SPEC IDS
  // ==========================================================================
  if (state.orphanedSpecIds.length > 0) {
    lines.push('## üîó Orphaned Spec IDs')
    lines.push('')
    lines.push(
      '> **Warning**: These spec IDs exist in E2E tests but are NOT linked to any user story.'
    )
    lines.push('> Every spec should trace back to a user requirement in `docs/user-stories/`.')
    lines.push('')
    lines.push('| Spec ID | File | Line | Test Name |')
    lines.push('|---------|------|------|-----------|')

    for (const orphan of state.orphanedSpecIds.slice(0, 50)) {
      const truncatedName =
        orphan.testName.length > 50 ? orphan.testName.substring(0, 47) + '...' : orphan.testName
      lines.push(
        `| \`${orphan.specId}\` | \`${orphan.file}\` | ${orphan.line} | ${truncatedName} |`
      )
    }

    if (state.orphanedSpecIds.length > 50) {
      lines.push('')
      lines.push(`... and ${state.orphanedSpecIds.length - 50} more orphaned spec IDs`)
    }
    lines.push('')
  }

  // ==========================================================================
  // TDD AUTOMATION
  // ==========================================================================
  if (state.tddAutomation.totalFixed > 0 || state.summary.totalFixme > 0) {
    lines.push('## ü§ñ TDD Automation')
    lines.push('')

    // Prose headers removed - ETA already in Executive Summary, details in table below

    lines.push('| Metric | Value |')
    lines.push('|--------|-------|')
    lines.push(`| Tests Fixed (90 days) | ${state.tddAutomation.totalFixed} |`)
    lines.push(`| ü§ñ Fixed by TDD Pipeline | ${state.tddAutomation.fixedByPipeline} |`)
    lines.push(`| üë§ Fixed Manually | ${state.tddAutomation.fixedManually} |`)
    lines.push(`| Fixed Last 24h | ${state.tddAutomation.fixedLast24h} |`)
    lines.push(`| Fixed Last 7d | ${state.tddAutomation.fixedLast7d} |`)
    lines.push(`| Fixed Last 30d | ${state.tddAutomation.fixedLast30d} |`)
    lines.push(`| Avg Fixes/Day | ${state.tddAutomation.avgFixesPerDay} |`)
    lines.push(`| Remaining | ${state.summary.totalFixme} |`)
    if (state.tddAutomation.estimatedDaysRemaining !== null) {
      lines.push(`| ETA | ${etaText} (${state.tddAutomation.estimatedCompletionDate || 'N/A'}) |`)
    }
    lines.push('')

    // Recent fixes
    if (state.tddAutomation.recentFixes.length > 0) {
      lines.push('<details>')
      lines.push('<summary>Recent Fixes (last 20)</summary>')
      lines.push('')
      lines.push('| Spec ID | Date | Commit | Source |')
      lines.push('|---------|------|--------|--------|')
      for (const fix of state.tddAutomation.recentFixes) {
        const sourceLabel = fix.source === 'tdd-pipeline' ? 'ü§ñ TDD' : 'üë§ Manual'
        lines.push(`| \`${fix.specId}\` | ${fix.date} | \`${fix.commitHash}\` | ${sourceLabel} |`)
      }
      lines.push('')
      lines.push('</details>')
      lines.push('')
    }
  }

  // ==========================================================================
  // ISSUES REQUIRING ATTENTION
  // ==========================================================================
  if (state.duplicateSpecIds.length > 0 || state.coverageGaps.length > 0 || allIssues.length > 0) {
    lines.push('## ‚ö° Issues Requiring Attention')
    lines.push('')

    // Duplicate Spec IDs
    if (state.duplicateSpecIds.length > 0) {
      lines.push('### ‚ùå Duplicate Spec IDs')
      lines.push('')
      lines.push('> **CRITICAL**: Each spec ID must be unique. Duplicates break TDD queue.')
      lines.push('')

      for (const dup of state.duplicateSpecIds) {
        lines.push(`**\`${dup.specId}\`** (${dup.locations.length} occurrences)`)
        for (const loc of dup.locations) {
          lines.push(`- \`${loc.file}:${loc.line}\``)
        }
        lines.push('')
      }
    }

    // Quality Issues
    if (allIssues.length > 0) {
      const suggestions = allIssues.filter((i) => i.type === 'suggestion')

      if (errors.length > 0) {
        lines.push('### ‚ùå Errors')
        lines.push('')
        for (const issue of errors) {
          const location = issue.line ? `:${issue.line}` : ''
          lines.push(`- **${issue.code}** in \`${issue.file}${location}\``)
          lines.push(`  ${issue.message}`)
        }
        lines.push('')
      }

      if (warnings.length > 0) {
        lines.push('### ‚ö†Ô∏è Warnings')
        lines.push('')
        for (const issue of warnings) {
          const location = issue.line ? `:${issue.line}` : ''
          lines.push(`- **${issue.code}** in \`${issue.file}${location}\``)
          lines.push(`  ${issue.message}`)
        }
        lines.push('')
      }

      if (suggestions.length > 0) {
        lines.push('<details>')
        lines.push('<summary>üí° Suggestions</summary>')
        lines.push('')
        for (const issue of suggestions) {
          lines.push(`- **${issue.code}** in \`${issue.file}\` - ${issue.message}`)
        }
        lines.push('')
        lines.push('</details>')
        lines.push('')
      }
    }

    // Coverage Gaps
    if (state.coverageGaps.length > 0) {
      lines.push('### üìâ Coverage Gaps')
      lines.push('')
      for (const gap of state.coverageGaps) {
        lines.push(`**${gap.feature}** - Missing:`)
        for (const behavior of gap.missingBehaviors) {
          lines.push(`- [ ] ${behavior}`)
        }
        lines.push('')
      }
    }
  }

  // ==========================================================================
  // FEATURE BREAKDOWN (with domain rollups)
  // ==========================================================================
  lines.push('## üìÅ Feature Breakdown')
  lines.push('')

  for (const domain of domainSummaries) {
    const domainProgressFilled = Math.round(domain.progressPercent / 5)
    const domainProgressEmpty = 20 - domainProgressFilled
    const statusLabel = getStatusLabel(
      domain.progressPercent,
      domain.files.some((f) => f.issues.length > 0)
    )

    lines.push(`### ${domain.domain}`)
    lines.push('')
    lines.push(
      `${domain.status} **${domain.totalFiles} files** | **${domain.totalTests} tests** | [${'‚ñà'.repeat(domainProgressFilled)}${'‚ñë'.repeat(domainProgressEmpty)}] ${domain.progressPercent}% | ${statusLabel}`
    )
    lines.push('')

    lines.push('<details>')
    lines.push(
      `<summary>View ${domain.domain} specs (${domain.totalFiles} files, ${domain.passingTests}/${domain.totalTests} passing)</summary>`
    )
    lines.push('')

    for (const file of domain.files) {
      const statusEmoji =
        file.metadata.fixmeTests > 0
          ? 'üöß'
          : file.issues.some((i) => i.type === 'error')
            ? '‚ùå'
            : file.issues.some((i) => i.type === 'warning')
              ? '‚ö†Ô∏è'
              : '‚úÖ'

      const fileProgress =
        file.metadata.totalTests > 0
          ? Math.round((file.metadata.passingTests / file.metadata.totalTests) * 100)
          : 100

      lines.push(`#### ${statusEmoji} ${file.feature} (${fileProgress}%)`)
      lines.push('')
      lines.push(`üìÅ \`${file.relativePath}\``)
      lines.push('')

      if (file.metadata.describeLabel) {
        lines.push(`**${file.metadata.describeLabel}**`)
        lines.push('')
      }

      // Compact stats with progress
      lines.push(
        `**${file.metadata.passingTests}/${file.metadata.totalTests}** passing | ${file.metadata.fixmeTests} fixme | ${file.metadata.specTests} @spec | ${file.metadata.regressionTests} @regression`
      )
      lines.push('')

      // Test list
      if (file.tests.length > 0) {
        lines.push('<details>')
        lines.push('<summary>Tests</summary>')
        lines.push('')

        for (const test of file.tests) {
          const statusIcon = test.isFixme ? '‚è∏Ô∏è' : '‚úÖ'
          const tagBadge = test.tag ? `\`${test.tag}\`` : ''
          const gwtStatus = [
            test.hasGiven ? 'G' : '~G~',
            test.hasWhen ? 'W' : '~W~',
            test.hasThen ? 'T' : '~T~',
          ].join('/')

          lines.push(`${statusIcon} **${test.id || 'NO-ID'}** ${tagBadge}`)
          lines.push(`   ${test.name}`)
          lines.push(`   \`${gwtStatus}\` Line ${test.lineNumber}`)
          lines.push('')
        }

        lines.push('</details>')
        lines.push('')
      }
    }

    lines.push('</details>')
    lines.push('')
  }

  // ==========================================================================
  // QUICK REFERENCE
  // ==========================================================================
  lines.push('## üìñ Quick Reference')
  lines.push('')
  lines.push('### Test Structure Template')
  lines.push('')
  lines.push('```typescript')
  lines.push("test('FEATURE-AREA-001: should [expected behavior] when [condition]',")
  lines.push("  { tag: '@spec' },")
  lines.push('  async ({ page, startServerWithSchema }) => {')
  lines.push('    // GIVEN: [initial context/setup]')
  lines.push('    await startServerWithSchema({ ... })')
  lines.push('')
  lines.push('    // WHEN: [action performed]')
  lines.push("    await page.goto('/')")
  lines.push('')
  lines.push('    // THEN: [expected outcome]')
  lines.push('    await expect(page.locator(...)).toBeVisible()')
  lines.push('  }')
  lines.push(')')
  lines.push('```')
  lines.push('')
  lines.push('### Spec ID Format')
  lines.push('')
  lines.push('| Prefix | Area |')
  lines.push('|--------|------|')
  lines.push('| `APP-` | Application schema specs |')
  lines.push('| `API-` | API endpoint specs |')
  lines.push('| `STATIC-` | Static generation specs |')
  lines.push('| `MIGRATION-` | Migration specs |')
  lines.push('')
  lines.push('### Commands')
  lines.push('')
  lines.push('```bash')
  lines.push('# Regenerate this file')
  lines.push('bun run progress')
  lines.push('')
  lines.push('# Run all specs')
  lines.push('bun test:e2e:spec')
  lines.push('')
  lines.push('# Run regression tests')
  lines.push('bun test:e2e:regression')
  lines.push('')
  lines.push('# Verify progress against GitHub')
  lines.push('bun run progress --verify-progress')
  lines.push('```')
  lines.push('')

  return lines.join('\n')
}

// =============================================================================
// Auto-fix helpers
// =============================================================================

/**
 * Fix header "Spec Count: X" to match actual spec count
 * @param filePath - Path to the spec file
 * @param actualCount - Correct spec count
 * @returns true if file was modified, false otherwise
 */
async function fixHeaderCount(filePath: string, actualCount: number): Promise<boolean> {
  const content = await readFile(filePath, 'utf-8')

  // Replace the spec count in the header
  const updatedContent = content.replace(/(\*\s*Spec Count:\s*)\d+/, `$1${actualCount}`)

  if (updatedContent !== content) {
    await writeFile(filePath, updatedContent)
    return true
  }
  return false
}

// =============================================================================
// Main
// =============================================================================

async function main() {
  const args = process.argv.slice(2)
  const filterPattern = args.find((a) => a.startsWith('--filter='))?.replace('--filter=', '')
  const showFixmeOnly = args.includes('--fixme')
  const noErrorExit = args.includes('--no-error') || !!filterPattern
  const verifyProgress = args.includes('--verify-progress')
  const shouldFix = args.includes('--fix')
  const shouldUpdateStories = args.includes('--update-stories')

  console.log('Analyzing spec files...')
  console.log('')

  // Find all spec files
  let specFiles = await findSpecFiles(SPECS_DIR)

  if (filterPattern) {
    // Escape special regex characters to prevent ReDoS attacks (CWE-400/CWE-730)
    const escapedPattern = filterPattern.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')
    const regex = new RegExp(escapedPattern)
    specFiles = specFiles.filter((f) => regex.test(f))
    console.log(`Filtered to ${specFiles.length} files matching: ${filterPattern}`)
  } else {
    console.log(`Found ${specFiles.length} spec files`)
  }

  // Analyze each file
  const analyzedFiles: SpecFile[] = []

  for (const filePath of specFiles) {
    const file = await analyzeFile(filePath)
    analyzedFiles.push(file)
  }

  // Calculate summary
  const totalTests = analyzedFiles.reduce((sum, f) => sum + f.metadata.totalTests, 0)
  const totalSpecs = analyzedFiles.reduce((sum, f) => sum + f.metadata.specTests, 0)
  const totalRegressions = analyzedFiles.reduce((sum, f) => sum + f.metadata.regressionTests, 0)
  const totalFixme = analyzedFiles.reduce((sum, f) => sum + f.metadata.fixmeTests, 0)
  const totalPassing = analyzedFiles.reduce((sum, f) => sum + f.metadata.passingTests, 0)

  const qualityScore = calculateQualityScore(analyzedFiles)
  const coverageGaps = detectCoverageGaps(analyzedFiles)
  const duplicateSpecIds = detectDuplicateSpecIds(analyzedFiles)
  const tddAutomation = await calculateTDDAutomationStats(totalFixme)

  // Collect fixme and passing spec IDs for user story analysis
  const fixmeSpecIds = new Set<string>()
  const passingSpecIds = new Set<string>()

  for (const file of analyzedFiles) {
    for (const test of file.tests) {
      if (test.id) {
        if (test.isFixme) {
          fixmeSpecIds.add(test.id)
        } else {
          passingSpecIds.add(test.id)
        }
      }
    }
  }

  // Parse user stories and calculate metrics
  console.log('Analyzing user stories...')
  const userStoryMetrics = await parseUserStoryFiles(analyzedFiles, fixmeSpecIds, passingSpecIds)
  const orphanedSpecIds = detectOrphanedSpecIds(analyzedFiles, userStoryMetrics)

  // Add duplicate spec ID errors to the respective files
  for (const dup of duplicateSpecIds) {
    for (const loc of dup.locations) {
      const file = analyzedFiles.find((f) => f.relativePath === loc.file)
      if (file) {
        file.issues.push({
          type: 'error',
          code: 'DUPLICATE_SPEC_ID',
          message: `Spec ID "${dup.specId}" is duplicated (also in: ${
            dup.locations
              .filter((l) => l.file !== loc.file)
              .map((l) => l.file)
              .join(', ') || 'same file'
          })`,
          line: loc.line,
          testId: dup.specId,
        })
      }
    }
  }

  // Recalculate error counts after adding duplicate errors
  const allIssuesWithDuplicates = analyzedFiles.flatMap((f) => f.issues)
  const errorsWithDuplicates = allIssuesWithDuplicates.filter((i) => i.type === 'error').length
  const warningsWithDuplicates = allIssuesWithDuplicates.filter((i) => i.type === 'warning').length
  const suggestionsWithDuplicates = allIssuesWithDuplicates.filter(
    (i) => i.type === 'suggestion'
  ).length

  const state: SpecState = {
    generatedAt: new Date().toISOString(),
    summary: {
      totalFiles: analyzedFiles.length,
      totalTests,
      totalSpecs,
      totalRegressions,
      totalFixme,
      totalPassing,
      qualityScore,
      duplicateSpecIds: duplicateSpecIds.length,
      orphanedSpecIds: orphanedSpecIds.length,
      issuesByType: {
        errors: errorsWithDuplicates,
        warnings: warningsWithDuplicates,
        suggestions: suggestionsWithDuplicates,
      },
    },
    files: analyzedFiles,
    coverageGaps,
    duplicateSpecIds,
    orphanedSpecIds,
    tddAutomation,
    userStoryMetrics,
  }

  // Auto-fix header count mismatches if --fix flag is present
  if (shouldFix) {
    console.log('')
    console.log('Auto-fixing header count mismatches...')

    const filesToFix = analyzedFiles.filter((file) =>
      file.issues.some((issue) => issue.code === 'HEADER_COUNT_MISMATCH')
    )

    if (filesToFix.length === 0) {
      console.log('No header count mismatches to fix')
    } else {
      let fixedCount = 0

      for (const file of filesToFix) {
        const actualCount = file.metadata.specTests

        const wasFixed = await fixHeaderCount(file.path, actualCount)
        if (wasFixed) {
          fixedCount++
          console.log(`  ‚úÖ Fixed: ${file.relativePath} (updated to ${actualCount})`)
        }
      }

      console.log('')
      console.log(`‚úÖ Fixed header count in ${fixedCount} file(s)`)
    }
  }

  // Update user story files with test status if --update-stories flag is present
  if (shouldUpdateStories) {
    console.log('')
    console.log('Updating user story files with test status...')

    const { updated, unchanged, changes } = await updateUserStoryFiles(fixmeSpecIds, passingSpecIds)

    if (updated === 0) {
      console.log('No user story files needed updates')
    } else {
      console.log('')
      for (const change of changes) {
        console.log(`  ‚úÖ Updated: ${change}`)
      }
      console.log('')
      console.log(`‚úÖ Updated ${updated} file(s), ${unchanged} unchanged`)
    }
  }

  // Generate markdown
  const markdown = generateMarkdown(state)

  // Format with Prettier and write output
  const prettierConfig = await prettier.resolveConfig(OUTPUT_FILE)
  const formattedMarkdown = await prettier.format(markdown, {
    ...prettierConfig,
    parser: 'markdown',
  })
  await writeFile(OUTPUT_FILE, formattedMarkdown)

  // Migration: Delete old SPEC-STATE.md if it exists
  try {
    await access(OLD_OUTPUT_FILE)
    await unlink(OLD_OUTPUT_FILE)
    console.log(`Deleted old file: SPEC-STATE.md (migrated to SPEC-PROGRESS.md)`)
  } catch {
    // File doesn't exist, no migration needed
  }

  // Update README badges with current progress
  const progressPercent = Math.round((totalPassing / Math.max(totalTests, 1)) * 100)
  const readmeContent = await readFile(README_FILE, 'utf-8')

  // URL-encode the badge values
  const specsBadge = `specs-${progressPercent}%25%20(${totalPassing}%2F${totalTests})-blue`
  const qualityBadge = `quality-${qualityScore}%25-brightgreen`
  const progressBadge = `progress-${progressPercent}%25-blue`

  // Update badges using regex - handles multiple formats
  const updatedReadme = readmeContent
    // Handle HTML img tag format: <img src="https://img.shields.io/badge/progress-XX%25-blue"
    .replace(
      /<img src="https:\/\/img\.shields\.io\/badge\/progress-\d+%25-blue"/,
      `<img src="https://img.shields.io/badge/${progressBadge}"`
    )
    // Handle markdown angle-bracket format: [![text](<url>)]
    .replace(
      /\[!\[Spec Progress\]\(<https:\/\/img\.shields\.io\/badge\/specs-[^>]+>\)\]/,
      `[![Spec Progress](<https://img.shields.io/badge/${specsBadge}>)]`
    )
    // Handle markdown regular format: [![text](url)]
    .replace(
      /\[!\[Spec Progress\]\(https:\/\/img\.shields\.io\/badge\/specs-[^)]+\)\]/,
      `[![Spec Progress](<https://img.shields.io/badge/${specsBadge}>)]`
    )
    .replace(
      /\[!\[Quality Score\]\(<https:\/\/img\.shields\.io\/badge\/quality-[^>]+>\)\]/,
      `[![Quality Score](https://img.shields.io/badge/${qualityBadge})]`
    )
    .replace(
      /\[!\[Quality Score\]\(https:\/\/img\.shields\.io\/badge\/quality-[^)]+\)\]/,
      `[![Quality Score](https://img.shields.io/badge/${qualityBadge})]`
    )
    // Migration: Update SPEC-STATE.md links to SPEC-PROGRESS.md
    .replace(/SPEC-STATE\.md/g, 'SPEC-PROGRESS.md')

  if (updatedReadme !== readmeContent) {
    const formattedReadme = await prettier.format(updatedReadme, {
      ...prettierConfig,
      parser: 'markdown',
    })
    await writeFile(README_FILE, formattedReadme)
    console.log(`‚úÖ Updated: ${README_FILE}`)
  }

  // Console output
  console.log('')
  console.log('‚îÅ'.repeat(60))
  console.log('üìä SPEC QUALITY REPORT')
  console.log('‚îÅ'.repeat(60))
  console.log('')
  console.log(`Quality Score: ${qualityScore}%`)
  console.log('')

  // User Stories metrics (shown first)
  if (userStoryMetrics && userStoryMetrics.totalStories > 0) {
    const notSpecifiedPercent = userStoryMetrics.storiesToSpecifyPercent
    const specifiedFixmePercent = userStoryMetrics.storiesSpecifiedNotImplementedPercent
    const specifiedPassingPercent = userStoryMetrics.storiesSpecifiedAndImplementedPercent
    console.log(`User Stories:    ${userStoryMetrics.totalStories} total`)
    console.log(
      `  ‚îú‚îÄ Not Specified:      ${userStoryMetrics.storiesToSpecify.length} (${notSpecifiedPercent}%)`
    )
    console.log(
      `  ‚îú‚îÄ Specified (Fixme):  ${userStoryMetrics.storiesSpecifiedNotImplemented.length} (${specifiedFixmePercent}%)`
    )
    console.log(
      `  ‚îî‚îÄ Specified (Passing): ${userStoryMetrics.storiesSpecifiedAndImplemented.length} (${specifiedPassingPercent}%)`
    )
    console.log('')
  }

  console.log(`Tests:           ${totalTests} total`)
  console.log(`  ‚îú‚îÄ @spec:       ${totalSpecs}`)
  console.log(`  ‚îú‚îÄ @regression: ${totalRegressions}`)
  console.log(`  ‚îú‚îÄ Passing:     ${totalPassing}`)
  console.log(`  ‚îî‚îÄ Fixme:       ${totalFixme}`)
  console.log('')
  const orphanedCount = orphanedSpecIds.length
  const totalIssuesWithOrphaned = allIssuesWithDuplicates.length + orphanedCount
  const errorsWithOrphaned = errorsWithDuplicates + orphanedCount
  console.log(`Issues:          ${totalIssuesWithOrphaned} total`)
  console.log(`  ‚îú‚îÄ Errors:      ${errorsWithOrphaned}`)
  console.log(`  ‚îú‚îÄ Warnings:    ${warningsWithDuplicates}`)
  console.log(`  ‚îî‚îÄ Suggestions: ${suggestionsWithDuplicates}`)
  console.log('')

  // TDD Automation stats
  if (tddAutomation.totalFixed > 0 || totalFixme > 0) {
    console.log('TDD Automation:')
    console.log(`  ‚îú‚îÄ Fixed (90d):  ${tddAutomation.totalFixed}`)
    console.log(`  ‚îÇ  ‚îú‚îÄ Pipeline:  ${tddAutomation.fixedByPipeline}`)
    console.log(`  ‚îÇ  ‚îî‚îÄ Manual:    ${tddAutomation.fixedManually}`)
    console.log(`  ‚îú‚îÄ Last 24h:     ${tddAutomation.fixedLast24h}`)
    console.log(`  ‚îú‚îÄ Last 7d:      ${tddAutomation.fixedLast7d}`)
    console.log(`  ‚îú‚îÄ Avg/Day:      ${tddAutomation.avgFixesPerDay}`)
    console.log(`  ‚îî‚îÄ Remaining:    ${totalFixme}`)
    if (tddAutomation.estimatedDaysRemaining !== null) {
      const days = tddAutomation.estimatedDaysRemaining
      const etaText =
        days === 0
          ? 'Less than 1 day'
          : days === 1
            ? '~1 day'
            : days < 7
              ? `~${days} days`
              : days < 30
                ? `~${Math.ceil(days / 7)} weeks`
                : `~${Math.ceil(days / 30)} months`
      console.log('')
      console.log(`ETA: ${etaText} (${tddAutomation.estimatedCompletionDate})`)
    }
    console.log('')
  }

  if (duplicateSpecIds.length > 0) {
    console.log('‚ùå DUPLICATE SPEC IDs:')
    for (const dup of duplicateSpecIds) {
      console.log(`   ${dup.specId} (${dup.locations.length} occurrences):`)
      for (const loc of dup.locations) {
        console.log(`     - ${loc.file}:${loc.line}`)
      }
    }
    console.log('')
  }

  if (errorsWithDuplicates > 0) {
    console.log('‚ùå ERRORS:')
    for (const file of analyzedFiles) {
      for (const issue of file.issues.filter((i) => i.type === 'error')) {
        console.log(`   ${file.relativePath}:${issue.line || '?'} - ${issue.code}`)
        console.log(`     ${issue.message}`)
      }
    }
    console.log('')
  }

  console.log('‚îÅ'.repeat(60))
  console.log(`‚úÖ Generated: ${OUTPUT_FILE}`)
  console.log('‚îÅ'.repeat(60))

  // Show fixme tests for prioritization
  if (showFixmeOnly) {
    console.log('')
    console.log('FIXME TESTS (Next to implement):')
    console.log('')
    for (const file of analyzedFiles) {
      const fixmeTests = file.tests.filter((t) => t.isFixme)
      if (fixmeTests.length > 0) {
        console.log(`  ${file.relativePath}`)
        for (const test of fixmeTests) {
          console.log(`    ${test.id || 'NO-ID'}: ${test.name.substring(0, 60)}...`)
        }
        console.log('')
      }
    }
  }

  // Verify progress against GitHub PRs/issues and commits
  if (verifyProgress) {
    console.log('')
    console.log('‚îÅ'.repeat(60))
    console.log('PROGRESS VERIFICATION (GitHub Cross-Reference)')
    console.log('‚îÅ'.repeat(60))
    console.log('')

    // Collect fixme and passing spec IDs from analyzed files
    const fixmeSpecIds = new Set<string>()
    const passingSpecIds = new Set<string>()

    for (const file of analyzedFiles) {
      for (const test of file.tests) {
        if (test.id) {
          if (test.isFixme) {
            fixmeSpecIds.add(test.id)
          } else {
            passingSpecIds.add(test.id)
          }
        }
      }
    }

    console.log(`Codebase Status:`)
    console.log(`   ‚îú‚îÄ Fixme spec IDs:   ${fixmeSpecIds.size}`)
    console.log(`   ‚îî‚îÄ Passing spec IDs: ${passingSpecIds.size}`)
    console.log('')

    console.log('Fetching GitHub references...')
    const verification = await verifyProgressFromGitHub(fixmeSpecIds, passingSpecIds)

    // Group references by type
    const prRefs = verification.allReferences.filter((r) => r.type === 'pr')
    const issueRefs = verification.allReferences.filter((r) => r.type === 'issue')
    const commitRefs = verification.allReferences.filter((r) => r.type === 'commit')

    // Deduplicate spec IDs across all sources
    const uniqueSpecIdsFromPRs = new Set(prRefs.map((r) => r.specId))
    const uniqueSpecIdsFromIssues = new Set(issueRefs.map((r) => r.specId))
    const uniqueSpecIdsFromCommits = new Set(commitRefs.map((r) => r.specId))

    console.log('')
    console.log(`GitHub References Found:`)
    console.log(
      `   ‚îú‚îÄ Merged PRs:        ${prRefs.length} (${uniqueSpecIdsFromPRs.size} unique spec IDs)`
    )
    console.log(
      `   ‚îú‚îÄ Closed Issues:     ${issueRefs.length} (${uniqueSpecIdsFromIssues.size} unique spec IDs)`
    )
    console.log(
      `   ‚îî‚îÄ Commits:           ${commitRefs.length} (${uniqueSpecIdsFromCommits.size} unique spec IDs)`
    )
    console.log('')
    console.log(`   Total unique spec IDs tracked: ${verification.fixedSpecIds.size}`)
    console.log('')

    // Show discrepancies
    if (verification.discrepancies.length > 0) {
      console.log('‚ö†Ô∏è  DISCREPANCIES DETECTED:')
      console.log('')
      for (const disc of verification.discrepancies) {
        console.log(`   ‚ùå ${disc.specId} (${disc.status})`)
        console.log(`      ${disc.suggestion}`)
        console.log(`      References:`)
        for (const ref of disc.references.slice(0, 3)) {
          switch (ref.type) {
            case 'pr':
              console.log(`        - PR #${ref.number}: ${ref.title.substring(0, 60)}...`)
              break
            case 'issue':
              console.log(`        - Issue #${ref.number}: ${ref.title.substring(0, 60)}...`)
              break
            case 'commit':
              console.log(`        - Commit ${ref.hash}: ${ref.title.substring(0, 60)}...`)
              break
          }
        }
        if (disc.references.length > 3) {
          console.log(`        ... and ${disc.references.length - 3} more references`)
        }
        console.log('')
      }

      console.log('‚îÅ'.repeat(60))
      console.log(`‚ùå Found ${verification.discrepancies.length} discrepancies`)
      console.log(
        '   These specs appear to be fixed in PRs/commits but are still marked as .fixme()'
      )
      console.log(
        '   Action: Remove .fixme() from these tests or investigate why they are still failing'
      )
      console.log('‚îÅ'.repeat(60))
    } else {
      console.log('‚úÖ No discrepancies found - all fixed specs are correctly tracked!')
    }
    console.log('')

    // Show recent fixes from GitHub for comparison
    const recentPRs = prRefs.slice(0, 10)
    if (recentPRs.length > 0) {
      console.log('Recent Merged PRs with Spec IDs (last 10):')
      for (const pr of recentPRs) {
        const dateStr = new Date(pr.date).toISOString().substring(0, 16).replace('T', ' ')
        console.log(`   PR #${pr.number} | ${dateStr} | ${pr.specId}`)
      }
      console.log('')
    }
  }

  // Exit with error code if there are errors (including duplicates) (unless --no-error or --filter is used)
  if (errorsWithDuplicates > 0 && !noErrorExit) {
    process.exit(1)
  }
}

main().catch((error) => {
  console.error('Fatal error:', error)
  process.exit(1)
})
